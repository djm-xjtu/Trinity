# questions1

- Parallelization: GPU run code that is inherently massively and embarrassingly parallel, which means there are few data dependencies and few branches. Through the processing units run as parallel and as fast as you can manage and don't care what the data in the neighboring units looks like. when CPU run code, it can only run one step after another because it compute values then decide what to do next with what data.
- Parallel programs are commonly written in DirectX Shaders, OpenCL, or CUDA. In general these will scale fairly well, even on a CPU (as they'd run on a CPU's AVX or AVX512 cores, achieving decent scaling). Graphics and some mathematical workloads are inherently parallelizable and scalable so perform well on dedicated architectures.
- Are CPU and GPUs all that there are? No. ASICs (Application Specific Integrated Circuits) commonly offer many of the benefits of both CPUs and GPUs. FPGAs would be one instantiation of an ASIC.

- GPUs are clocked WAAY lower than CPU. Generally the lower the clocks, the better the scaling.
- IPC(Instructions per cycle) increases with about the square root of the area, so this means that making a core twice as big would only make it about 41% faster. Thee would be knock on power and other effects too.
- CPUs have stayed pretty stable on die size. Transistors increase, but process shrinkage has kept die size mostly stable. GPU die sizes have exploded in the past couple years
- DSRC(Dedicated Short Range Communications) is wireless both-direction radiocommunication technology.

# questions2

- Amdahl's law

the overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used. speedup = 1 / (s + p / N). s is the proportion of execution time spent on the serial part, p is the proportion of execution time spent on the part that can be parallelized, and N is the number of processors. 

- Gustafson's law

the scaled speedup increases linearly with respect to the number of processors (with a slope smaller than one), and there is no upper limit for the scaled speedup. scaled speedup = s + p × N

- ASICs

an ASIC is designed specifically for its intended application. It has only the blocks required for optimum operation. It’s best-suited for high-volume applications. The Tensor Processing Unit (TPU) by Google, for example, is an accelerator specifically for neural-network machine learning.

- FPGA

FPGAs offer internal hardware blocks with user-programmable interconnects to customize operations for a specific applications. As the connections between blocks can readily be reprogrammed, changing the internal operation of the hardware, this enables an FPGA to accommodate changes to a design, or even support a new application, during the lifetime of the part.  This makes them attractive for changing and evolving technology settings, such as digital television, consumer electronics, cybersecurity systems and wireless communications.

- Voltage Frequency curve

Dynamic voltage and frequency scaling is a technique in computer architecture whereby the frequency/voltage of a microprocessor can be automatically adjusted "on the fly" depending on the actual needs, to conserve power and reduce the amount of heat generated by the chip



# question3

- 